{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping with cbs_utils\n",
    "\n",
    "In this notebook some small examples are given on how to use the web scraping utilities from cbs_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the get_page_from_url function\n",
    "\n",
    "The *get_page_from_url* function allow to obtain the contents of an url and store the results in cache. The next time you run the function again, the function is read from cache. Here, an small example is given. First start with import the required modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from cbs_utils.misc import (create_logger, merge_loggers)\n",
    "from cbs_utils.regular_expressions import (KVK_REGEXP, ZIP_REGEXP, BTW_REGEXP)\n",
    "from cbs_utils.web_scraping import (get_page_from_url, UrlSearchStrings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the logging module using the cbs_utils misc function *create_logger*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger cbs_utils.web_scraping (INFO)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up logging\n",
    "log_level = logging.DEBUG  # change to DEBUG for more info\n",
    "log_format = logging.Formatter('%(levelname)8s --- %(message)s')\n",
    "logger = create_logger(console_log_level=log_level, formatter=log_format)\n",
    "merge_loggers(logger, \"cbs_utils.web_scraping\", logger_level_to_merge=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example a *tmp* directory is made in your working directory to store the cache. First make sure we clean this directory in case it still existed from the previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create url name and clean previous cache file\n",
    "cache_directory = Path(\"tmp\")\n",
    "clean_cache = True\n",
    "if clean_cache:\n",
    "    if cache_directory.exists():\n",
    "        for item in cache_directory.iterdir():\n",
    "            item.unlink()\n",
    "        cache_directory.rmdir()\n",
    "    else:\n",
    "        logger.info(f\"Cache directory {cache_directory} was already removed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can demonstrate the *get_page_from_url* function. Note that the *with Timer* construct is only added to be able to report the processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35 ms, sys: 3 ms, total: 38 ms\n",
      "Wall time: 5.43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "url = \"https://www.example.com\"\n",
    "page = get_page_from_url(url, cache_directory=cache_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it took about 5.5 s to get all the information from the internet. Because we have added a *cache_to_disk* iterator to the *get_page_from_url* function, a cache wil in the *tmp* directory was made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INFO --- Cache file: tmp/get_page_from_url_https_www_example_com_.pkl\n"
     ]
    }
   ],
   "source": [
    "for item in cache_directory.iterdir():\n",
    "    logger.info(f\"Cache file: {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contents of the url was stored in page and look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INFO --- \n",
      "<body>\n",
      "<div>\n",
      "<h1>Example Domain</h1>\n",
      "<p>This domain is established to be used for illustrative examples in documents. You may use this\n",
      "    domain in examples without prior coordination or asking for permission.</p>\n",
      "<p><a href=\"http://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(page.text, 'lxml')\n",
    "logger.info(\"\\n{}\\n\".format(soup.body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same function again. Since we now have a cache file, it will be about 1000 x faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1 ms, total: 1 ms\n",
      "Wall time: 783 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "page2 = get_page_from_url(url, cache_directory=cache_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed the contecnt with exactely the same function statement runs in with about 1 ms. Now compare the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INFO --- Contents is equal: True\n"
     ]
    }
   ],
   "source": [
    "soup2 = BeautifulSoup(page2.text, 'lxml')\n",
    "logger.info(\"Contents is equal: {}\".format(soup.body == soup2.body))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the *UrlSearchStrings* class\n",
    "\n",
    "The *UrlSearchString* class can be used to recursively crawl a web site and search for a list of regular expression we want to obtained from the web site. Again, the result is cached, so in case you want to run it again with different search string it will run significantly faster. \n",
    "\n",
    "Let first set up our first search session, trying to retrieve the postal code and kvk number from a web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    INFO --- Get (cached) page: https://www.be-one.nl/ with validate True (web_scraping.py:820)\n",
      "    INFO --- Get (cached) page: https://www.be-one.nl/ with validate True\n",
      "    INFO --- Get (cached) page: https://www.be-one.nl/ with validate True\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# the regular expression are obtained from the cbs_utils.regular_expressions module\n",
    "searches = dict(\n",
    "    postcode=ZIP_REGEXP,\n",
    "    kvknumber=KVK_REGEXP\n",
    ")\n",
    "\n",
    "url = \"www.be-one.nl\"\n",
    "url_analyse = UrlSearchStrings(url, search_strings=searches, cache_directory=cache_directory,\n",
    "                               store_page_to_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
